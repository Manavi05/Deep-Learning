{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNujm6dxFPDFBXI80xBbDFj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kHW-Wqhi2i2t","executionInfo":{"status":"ok","timestamp":1737211206190,"user_tz":-330,"elapsed":2443,"user":{"displayName":"Manavi Pawar","userId":"07315180676046614216"}},"outputId":"629a1c27-80e4-493d-c8be-f5170810346f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0, Loss: 0.7713\n","Epoch 1000, Loss: 0.6931\n","Epoch 2000, Loss: 0.6928\n","Epoch 3000, Loss: 0.6922\n","Epoch 4000, Loss: 0.6902\n","Epoch 5000, Loss: 0.6773\n","Epoch 6000, Loss: 0.5458\n","Epoch 7000, Loss: 0.2760\n","Epoch 8000, Loss: 0.1703\n","Epoch 9000, Loss: 0.1269\n","\n","Testing the neural network:\n","Input: [0 0], Predicted: 0.0962, Actual: 0\n","Input: [0 1], Predicted: 0.8939, Actual: 1\n","Input: [1 0], Predicted: 0.8941, Actual: 1\n","Input: [1 1], Predicted: 0.0856, Actual: 0\n"]}],"source":["import numpy as np\n","\n","# Sigmoid activation function and its derivative\n","def sigmoid(x):\n","    return 1 / (1 + np.exp(-x))\n","\n","def sigmoid_derivative(x):\n","    return x * (1 - x)\n","\n","# Initialize dataset\n","X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Input\n","Y = np.array([[0], [1], [1], [0]])              # Output\n","\n","# Initialize parameters\n","input_neurons = 2\n","hidden_neurons = 2\n","output_neurons = 1\n","learning_rate = 0.1\n","\n","# Randomly initialize weights and biases\n","np.random.seed(42)\n","W1 = np.random.uniform(-1, 1, (input_neurons, hidden_neurons))\n","b1 = np.zeros((1, hidden_neurons))\n","W2 = np.random.uniform(-1, 1, (hidden_neurons, output_neurons))\n","b2 = np.zeros((1, output_neurons))\n","\n","# Training the neural network\n","epochs = 10000\n","for epoch in range(epochs):\n","    # Forward pass\n","    Z1 = np.dot(X, W1) + b1\n","    A1 = sigmoid(Z1)\n","    Z2 = np.dot(A1, W2) + b2\n","    A2 = sigmoid(Z2)\n","\n","    # Compute loss (binary cross-entropy)\n","    loss = -np.mean(Y * np.log(A2) + (1 - Y) * np.log(1 - A2))\n","\n","    # Backpropagation\n","    dA2 = A2 - Y\n","    dZ2 = dA2 * sigmoid_derivative(A2)\n","    dW2 = np.dot(A1.T, dZ2)\n","    db2 = np.sum(dZ2, axis=0, keepdims=True)\n","\n","    dA1 = np.dot(dZ2, W2.T)\n","    dZ1 = dA1 * sigmoid_derivative(A1)\n","    dW1 = np.dot(X.T, dZ1)\n","    db1 = np.sum(dZ1, axis=0, keepdims=True)\n","\n","    # Update weights and biases\n","    W2 -= learning_rate * dW2\n","    b2 -= learning_rate * db2\n","    W1 -= learning_rate * dW1\n","    b1 -= learning_rate * db1\n","\n","    # Print loss every 1000 epochs\n","    if epoch % 1000 == 0:\n","        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n","\n","# Testing the neural network\n","print(\"\\nTesting the neural network:\")\n","for i in range(len(X)):\n","    Z1 = np.dot(X[i], W1) + b1\n","    A1 = sigmoid(Z1)\n","    Z2 = np.dot(A1, W2) + b2\n","    A2 = sigmoid(Z2)\n","    print(f\"Input: {X[i]}, Predicted: {A2[0][0]:.4f}, Actual: {Y[i][0]}\")\n"]}]}